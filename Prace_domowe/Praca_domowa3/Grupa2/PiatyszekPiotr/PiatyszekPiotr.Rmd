---
title: "WUM PD 3"
author: "Piotr Piątyszek"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
opts_chunk$set(fig.width = 10)
opts_chunk$set(fig.height = 8)
opts_chunk$set(warning  = FALSE)
opts_chunk$set(message  = FALSE)
opts_chunk$set(cache = TRUE)
```

## Read data
```{r read}
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
library(gbm)
library(caret)
library(pROC)
library(plyr)
library(mboost)
australia <- read.csv("./australia.csv") %>%
  mutate(target=as.factor(mapvalues(RainTomorrow, from=0:1, to=c("sun", "rain")))) %>%
  select(-RainTomorrow)
table(australia$target)
```

## Split
```{r split, dependson="read"}
set.seed(1515)
partition.index <- createDataPartition(australia$target, p = 0.6, list = FALSE)
data.train <- australia[partition.index, ]
data.test <- australia[-partition.index, ]
table(data.train$target)
table(data.test$target)
```

## Wybór metryki
Istnieje wiele metryk do oceny skuteczności modeli. W tym problemie przyjmuję, że błędna klasyfikacja niesie za sobą taki sam koszt niezależnie czy model błędnie przewidzi deszcz czy słońce. Jest to czysto intuicyjne założenie, wynikające z tego, że irytacja potencjalnego użytkownika naszej prognozy pogody będzie podobna, gdy go zaskoczy niespodziewany deszcz oraz gdy odwoła piknik ze znajomymi, a deszcz nie spadnie. Jednak dzięki temu możemy ograniczyć dostepne metryki, odrzucamy **sensivity**, **specyfity** jako metryki które preferują jedną z klas. Rozsądne wydaje się **Accuracy**, szczególnie **Balanced Accuracy** z uwagi na niezbalansowane klasy w zbiorze. Wada **Accuracy** polega na tym, że uwzględnia jedynie poprawne przypisania klasy, a nie prawdopodobieństwo danego przypisania. Jest mierzone dla domyślej wartości odcięcia. Metryka **AUC** (w caret czasami nazwywana **ROC**) rozwiązuje ten problem, ponieważ zwraca pole pod krzywą **Sensivity** od **(1-Specifity)** dla różnych wartości cutoff. Popularna jest też metryka **Kappa**, która skupia się na tym, by pokazać o ile lepszy jest nasz model od modelu losowego. Ponieważ jednak my porównujemy różne modele na tym samym zbiorze danych, to odniesienie do modelu losowego nie jest takie istotne, a zalety **AUC** przeważają.

### Funkcje pomocnicze
```{r metric, dependson="split"}
mySummary <- function(data, lev = NULL, model = NULL) {
  out <- c(
    defaultSummary(data, lev, model),
    twoClassSummary(data, lev, model)
  )
  out["Balanced Accuracy"] <- (out["Sens"] + out["Spec"])/2
  out
}
summaryOnTest <- function(model) {
  df <- data.frame(
    pred = as.character(predict(model, data.test)),
    obs = as.character(data.test$target)
  )
  df <- cbind(df, predict(model, data.test, type="prob"))
  mySummary(df, model$levels, model)
}
compareWithTest <- function(model) {
  testMetrics <- summaryOnTest(model)
  trainMetrics <- model$results %>% inner_join(model$bestTune)
  df <- rbind(trainMetrics[1, names(testMetrics)], testMetrics)
  rownames(df) <- c("Train", "Test")
  df
}
plotROC <- function(model) {
  rocObject <- pROC::roc(
    response = model$pred$obs,
    predictor = model$pred[, model$levels[1]],
    direction = ">",
    quite = TRUE
  )
  ggroc(rocObject)
}
ctrl <- trainControl(
  method = "repeatedcv",
  number = 5, # 5 fold cv
  repeats = 3, # repeated 3 times
  classProbs = TRUE,
  sampling = "up",
  summaryFunction = mySummary,
  savePredictions = "final",
  allowParallel=T
)
models <-list()
```

# Boosted Generalized Linear Model 
GLM z boostingiem oferuje dwa hiperparametry  
**mstop** - liczbę iteracji boostingu  
**prune** - czy ustawić mstop automatycznie  
Nie jestem pewien czy **prune** można nazwać hiperparametrem, natomiast niezaleznie od tego pozostawimy go na FALSE, ponieważ jego użycie zawsze kończyło się błędem, którego nie umiałem naprawić. Będziemy analizować tylko **mstop**.
```{r model, dependson="metric"}
tuneGrid <- expand.grid(mstop = c(10, 30, 60, 100, 150, 200, 300, 500, 1000), prune=FALSE)
cl <- makePSOCKcluster(3)
registerDoParallel(cl)
models$glmboost <- train(
  target ~ .,
  data = data.train,
  method = "glmboost",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = tuneGrid
)
stopCluster(cl)
```
### Metryki
```{r, cache=FALSE}
models$glmboost$results %>% arrange(desc(ROC)) %>% (knitr::kable)
```

### Wpływ mstop na AUC (tutaj nazwane ROC)
```{r, cache=FALSE}
plot(models$glmboost)
```

### Krzywa ROC dla finalnego modelu
```{r, cache=FALSE}
plotROC(models$glmboost)
```

### Weryfikacja metryk na danych testowych
```{r, cache=FALSE}
compareWithTest(models$glmboost) %>% (knitr::kable)
```

# Random Forest
Model lasów losowych oferyje jeden hiperparametr  
**mtry** - liczba losowo wybranych zmiennych, które będą brane pod uwagę przy każdym podziale drzewa  
```{r model_rf, dependson="metric"}
tuneGrid <- expand.grid(mtry = c(2, 5, 10))
cl <- makePSOCKcluster(3)
registerDoParallel(cl)
models$randomForest <- train(
  target ~ .,
  data = data.train,
  method = "rf",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = tuneGrid
)
stopCluster(cl)
```
### Metryki
```{r, cache=FALSE}
models$randomForest$results %>% arrange(desc(ROC)) %>% (knitr::kable)
```

### Wpływ mtry na AUC (tutaj nazwane ROC)
```{r, cache=FALSE}
plot(models$randomForest)
```

### Krzywa ROC dla finalnego modelu
```{r, cache=FALSE}
plotROC(models$randomForest)
```

### Weryfikacja metryk na danych testowych
```{r, cache=FALSE}
compareWithTest(models$randomForest) %>% (knitr::kable)
```

# Stochastic Gradient Boosting
GBM oferuje kilka hiperparametrów  
**n.trees** - liczba iteracji boostingu  
**interaction.depth** - liczba podziałów drzewa  
**shrinkage** - współczynnik uczenia się, tym mniejsyz tym bardziej ogranicza wpływ każdej iteracji, wydłużając uczenie  
**n.minobsinnode** - minimalna liczba obserwacji w liściu drzewa  
```{r model_gbm, dependson="metric", results="hide"}
tuneGrid <- expand.grid(
  interaction.depth = c(1, 3, 5), 
  n.trees = (1:30)*10, 
  shrinkage = c(0.1, 0.05),
  n.minobsinnode = c(5, 10, 20)
)
cl <- makePSOCKcluster(3)
registerDoParallel(cl)
models$gbm <- train(
  target ~ .,
  data = data.train,
  method = "gbm",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = tuneGrid
)
stopCluster(cl)
```
### Metryki
```{r, cache=FALSE}
models$gbm$results %>% arrange(desc(ROC)) %>% head(n=20) %>% (knitr::kable)
```

### Wpływ hiperparametrów na AUC (tutaj nazwane ROC)
```{r, cache=FALSE}
plot(models$gbm)
```

### Krzywa ROC dla finalnego modelu
```{r, cache=FALSE}
plotROC(models$gbm)
```

### Weryfikacja metryk na danych testowych
```{r, cache=FALSE}
compareWithTest(models$gbm) %>% (knitr::kable)
```

# Porównanie klasyfikatorów
```{r, cache=FALSE}
do.call('rbind', lapply(models, function(model) {
  compareWithTest(model)["Test",]
})) %>% (knitr::kable)
```

Jak widać po metryce AUC(ROC), wszystkie modele osiągneły bardzo zbliżone wyniki. Najlepszy osiągnął las losowy, choć należy zauważyć, że prawie identyczne pole pod krzywą ma GBM, jednak jest znacznie bardziej symetryczny (Sensivity vs Specifity), co przekłada się też na wynik zbalansowego accuracy. Z tego powodu GBM wydaje mi się najbardziej rozsądnym wyborem, choć GLM z boostingiem też nie odstaje wynikiem, a jednak wymaga znacznie mniejszej mocy obliczeniowej.
